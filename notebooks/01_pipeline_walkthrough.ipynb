{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCR-NLP Pipeline Walkthrough\n",
    "\n",
    "This notebook demonstrates how to use the OCR-NLP Pipeline for extracting structured information from documents like PDFs, invoices, and forms.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's import the necessary modules and set up the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "# Add the parent directory to the path so we can import the modules\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import the pipeline modules\n",
    "from src.pipeline import PipelineBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating a Pipeline\n",
    "\n",
    "The pipeline is built using the `PipelineBuilder` class, which allows for easy configuration of the OCR engine, NLP processing, and entity extraction components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline with default configuration\n",
    "pipeline = PipelineBuilder().build()\n",
    "\n",
    "# Alternatively, you can customize the pipeline\n",
    "custom_pipeline = (\n",
    "    PipelineBuilder()\n",
    "    .with_ocr_engine('tesseract', lang='eng')\n",
    "    .with_ocr_dpi(300)\n",
    "    .with_language('en')\n",
    "    .with_entity_extractor('spacy', model_name='en_core_web_sm')\n",
    "    .build()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Processing a Document\n",
    "\n",
    "Now let's process a sample document. For this demo, we'll use one of the sample PDFs included in the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to sample document\n",
    "sample_dir = Path('../data/pdf_samples')\n",
    "output_dir = Path('../outputs')\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# List available sample documents\n",
    "sample_docs = list(sample_dir.glob('*.pdf'))\n",
    "print(f\"Available sample documents: {[doc.name for doc in sample_docs]}\")\n",
    "\n",
    "# Select a sample document (e.g., an invoice)\n",
    "sample_doc = next((doc for doc in sample_docs if 'invoice' in doc.name.lower()), sample_docs[0])\n",
    "print(f\"Selected document: {sample_doc.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on OCR Dependencies\n",
    "\n",
    "For a full OCR processing pipeline, you would need to have Tesseract OCR installed on your system. Since this is a demo, we'll use pre-processed sample outputs to demonstrate the pipeline's capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-processed sample output\n",
    "sample_output_path = Path(f'../data/sample_outputs/{sample_doc.stem}_results.json')\n",
    "\n",
    "if sample_output_path.exists():\n",
    "    with open(sample_output_path, 'r', encoding='utf-8') as f:\n",
    "        result = json.load(f)\n",
    "    print(f\"Loaded pre-processed results from {sample_output_path}\")\n",
    "else:\n",
    "    print(f\"No pre-processed results found for {sample_doc.name}\")\n",
    "    print(\"To generate sample outputs, run: python scripts/generate_sample_outputs.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyzing the Results\n",
    "\n",
    "Now let's examine the results of the OCR and entity extraction process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display document metadata\n",
    "print(\"Document Metadata:\")\n",
    "pprint(result['document'])\n",
    "\n",
    "# Display OCR text sample (first 200 characters)\n",
    "print(\"\\nOCR Text Sample:\")\n",
    "print(result['ocr']['text'][:200] + \"...\")\n",
    "\n",
    "# Display text analysis\n",
    "print(\"\\nText Analysis:\")\n",
    "pprint(result['text_analysis'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploring Extracted Entities\n",
    "\n",
    "One of the key features of the pipeline is entity extraction. Let's examine the entities that were extracted from the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group entities by type\n",
    "entities_by_type = {}\n",
    "for entity in result['entities']:\n",
    "    entity_type = entity['type']\n",
    "    if entity_type not in entities_by_type:\n",
    "        entities_by_type[entity_type] = []\n",
    "    entities_by_type[entity_type].append(entity)\n",
    "\n",
    "# Display entities by type\n",
    "print(f\"Extracted Entities ({len(result['entities'])}):\\n\")\n",
    "for entity_type, entities in entities_by_type.items():\n",
    "    print(f\"{entity_type} ({len(entities)})\")\n",
    "    for entity in entities[:3]:  # Show up to 3 entities per type\n",
    "        print(f\"  - {entity['text']} (confidence: {entity['confidence']})\")\n",
    "    \n",
    "    if len(entities) > 3:\n",
    "        print(f\"  - ... and {len(entities) - 3} more\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizing Entity Extraction\n",
    "\n",
    "Let's create a simple visualization of the extracted entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Count entities by type\n",
    "entity_types = list(entities_by_type.keys())\n",
    "entity_counts = [len(entities_by_type[t]) for t in entity_types]\n",
    "\n",
    "# Create a bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(entity_types, entity_counts, color='skyblue')\n",
    "plt.xlabel('Entity Type')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Extracted Entities by Type')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Add count labels on top of bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "             f'{height}', ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Creating a Structured Document Summary\n",
    "\n",
    "Now let's create a structured summary of the document based on the extracted entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_document_summary(entities_by_type):\n",
    "    \"\"\"Create a structured summary of the document based on extracted entities.\"\"\"\n",
    "    summary = {}\n",
    "    \n",
    "    # Extract document type\n",
    "    if 'INVOICE_NUM' in entities_by_type:\n",
    "        summary['document_type'] = 'Invoice'\n",
    "        \n",
    "        # Extract invoice details\n",
    "        summary['invoice_number'] = entities_by_type['INVOICE_NUM'][0]['text'] if entities_by_type['INVOICE_NUM'] else None\n",
    "        summary['date'] = entities_by_type['DATE'][0]['text'] if 'DATE' in entities_by_type else None\n",
    "        summary['total_amount'] = entities_by_type['TOTAL'][0]['text'] if 'TOTAL' in entities_by_type else None\n",
    "        \n",
    "        # Extract vendor/customer information\n",
    "        summary['vendor'] = entities_by_type['ORG'][0]['text'] if 'ORG' in entities_by_type else None\n",
    "        summary['customer'] = entities_by_type['PERSON'][0]['text'] if 'PERSON' in entities_by_type else None\n",
    "        \n",
    "        # Extract line items\n",
    "        if 'PRODUCT' in entities_by_type and 'QUANTITY' in entities_by_type and 'MONEY' in entities_by_type:\n",
    "            summary['line_items'] = []\n",
    "            for i in range(min(len(entities_by_type['PRODUCT']), len(entities_by_type['QUANTITY']), len(entities_by_type['MONEY']))):\n",
    "                summary['line_items'].append({\n",
    "                    'product': entities_by_type['PRODUCT'][i]['text'],\n",
    "                    'quantity': entities_by_type['QUANTITY'][i]['text'],\n",
    "                    'price': entities_by_type['MONEY'][i]['text']\n",
    "                })\n",
    "    elif 'PERSON' in entities_by_type and 'ADDRESS' in entities_by_type:\n",
    "        summary['document_type'] = 'Form'\n",
    "        \n",
    "        # Extract form details\n",
    "        summary['person'] = entities_by_type['PERSON'][0]['text'] if entities_by_type['PERSON'] else None\n",
    "        summary['address'] = entities_by_type['ADDRESS'][0]['text'] if entities_by_type['ADDRESS'] else None\n",
    "        summary['date'] = entities_by_type['DATE'][0]['text'] if 'DATE' in entities_by_type else None\n",
    "        summary['organization'] = entities_by_type['ORG'][0]['text'] if 'ORG' in entities_by_type else None\n",
    "        summary['tax_id'] = entities_by_type['TAX_ID'][0]['text'] if 'TAX_ID' in entities_by_type else None\n",
    "    else:\n",
    "        summary['document_type'] = 'Unknown'\n",
    "        \n",
    "    return summary\n",
    "\n",
    "# Create and display document summary\n",
    "document_summary = create_document_summary(entities_by_type)\n",
    "print(\"Document Summary:\")\n",
    "pprint(document_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exporting Structured Data\n",
    "\n",
    "Finally, let's export the structured data to a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export document summary to JSON\n",
    "summary_output_path = output_dir / f\"{sample_doc.stem}_summary.json\"\n",
    "with open(summary_output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(document_summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Document summary exported to: {summary_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to use the OCR-NLP Pipeline to extract structured information from documents. The pipeline combines OCR, text preprocessing, and entity extraction to convert unstructured documents into structured data that can be easily analyzed and processed.\n",
    "\n",
    "Key features demonstrated:\n",
    "- Creating and configuring the pipeline\n",
    "- Processing documents\n",
    "- Analyzing extracted entities\n",
    "- Visualizing entity extraction results\n",
    "- Creating structured document summaries\n",
    "\n",
    "For more advanced usage, refer to the documentation and examples in the repository."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
