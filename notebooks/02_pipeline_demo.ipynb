{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 – Full OCR-NLP Pipeline Demo\n",
    "\n",
    "This notebook demonstrates the complete pipeline: PDF → OCR → NLP → Entity Recognition → JSON Output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "# Add the parent directory to the path so we can import our modules\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.pipeline import PipelineBuilder\n",
    "\n",
    "# Define paths\n",
    "RAW_DIR = Path(\"../data/raw\")\n",
    "OUTPUT_DIR = Path(\"../outputs\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build the Pipeline\n",
    "\n",
    "First, we'll create a pipeline with our desired configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a pipeline with custom configuration\n",
    "pipeline = (\n",
    "    PipelineBuilder()\n",
    "    .with_ocr_engine('tesseract', lang='eng')\n",
    "    .with_ocr_dpi(300)\n",
    "    .with_language('en')\n",
    "    .with_entity_extractor('spacy', model_name='en_core_web_sm')\n",
    "    # Add custom patterns for domain-specific entities\n",
    "    .with_entity_extractor('custom', patterns={\n",
    "        'EMAIL': [r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'],\n",
    "        'PHONE': [r'\\+?[\\d\\-\\(\\)\\s]{10,20}'],\n",
    "        'URL': [r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'],\n",
    "        'INVOICE_NUMBER': [r'(?:Invoice|INV)\\s*#?\\s*(\\d+)', r'#\\s*(\\d+)'],\n",
    "        'AMOUNT': [r'\\$\\s*[\\d,]+\\.?\\d*']\n",
    "    })\n",
    "    .build()\n",
    ")\n",
    "\n",
    "print(\"✅ Pipeline initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Process a Sample Document\n",
    "\n",
    "Let's process a sample document through the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check if we have any sample PDFs\n",
    "sample_pdfs = list(RAW_DIR.glob(\"*.pdf\"))\n",
    "\n",
    "if not sample_pdfs:\n",
    "    # If no PDFs found, use the sample image from the first notebook\n",
    "    sample_files = list(Path(\"../data/samples\").glob(\"*.png\"))\n",
    "    if sample_files:\n",
    "        sample_path = sample_files[0]\n",
    "        print(f\"Using sample image: {sample_path}\")\n",
    "    else:\n",
    "        print(\"No sample files found. Please add a PDF or image to the data/raw or data/samples directory.\")\n",
    "        sample_path = None\n",
    "else:\n",
    "    sample_path = sample_pdfs[0]\n",
    "    print(f\"Using sample PDF: {sample_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if sample_path:\n",
    "    # Process the document\n",
    "    result = pipeline.process_document(\n",
    "        sample_path,\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        preprocess_ocr=True,\n",
    "        clean_text=True\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Processed {sample_path.name}\")\n",
    "    print(f\"Found {result['entity_extraction']['count']} entities\")\n",
    "    print(f\"Total processing time: {result['total_processing_time']:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Examine the Extracted Entities\n",
    "\n",
    "Let's look at the entities that were extracted from the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if 'entities' in result and result['entities']:\n",
    "    # Group entities by type\n",
    "    entities_by_type = {}\n",
    "    for entity in result['entities']:\n",
    "        entity_type = entity['label']\n",
    "        if entity_type not in entities_by_type:\n",
    "            entities_by_type[entity_type] = []\n",
    "        entities_by_type[entity_type].append(entity['text'])\n",
    "    \n",
    "    # Print entities by type\n",
    "    print(\"Extracted Entities:\")\n",
    "    for entity_type, entities in entities_by_type.items():\n",
    "        print(f\"\\n{entity_type}:\")\n",
    "        for entity in entities:\n",
    "            print(f\"  - {entity}\")\n",
    "else:\n",
    "    print(\"No entities found in the document.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize the JSON Output\n",
    "\n",
    "Let's look at the structured JSON output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a simplified JSON output with just the key information\n",
    "if sample_path:\n",
    "    simplified_output = {\n",
    "        \"document\": result[\"document\"][\"name\"],\n",
    "        \"entities\": result[\"entities\"],\n",
    "        \"word_count\": result[\"text_analysis\"][\"word_count\"],\n",
    "        \"processing_time\": result[\"total_processing_time\"]\n",
    "    }\n",
    "    \n",
    "    # Save the simplified output\n",
    "    simplified_path = OUTPUT_DIR / f\"{sample_path.stem}_simplified.json\"\n",
    "    with open(simplified_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(simplified_output, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\nSaved simplified output to: {simplified_path}\")\n",
    "    \n",
    "    # Display the first few entities\n",
    "    print(\"\\nSample of extracted entities:\")\n",
    "    for entity in result[\"entities\"][:5]:  # Show first 5 entities\n",
    "        print(f\"- {entity['text']} ({entity['label']})\")\n",
    "    \n",
    "    if len(result[\"entities\"]) > 5:\n",
    "        print(f\"... and {len(result['entities']) - 5} more entities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Batch Processing\n",
    "\n",
    "The pipeline can also process multiple documents in batch mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Find all documents in the raw directory\n",
    "all_docs = list(RAW_DIR.glob(\"*.pdf\")) + list(RAW_DIR.glob(\"*.png\")) + list(RAW_DIR.glob(\"*.jpg\"))\n",
    "\n",
    "if len(all_docs) > 1:\n",
    "    print(f\"Found {len(all_docs)} documents for batch processing\")\n",
    "    \n",
    "    # Process all documents\n",
    "    batch_results = pipeline.process_batch(\n",
    "        all_docs,\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        preprocess_ocr=True,\n",
    "        clean_text=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\nBatch Processing Summary:\")\n",
    "    for i, result in enumerate(batch_results):\n",
    "        doc_name = result.get('document', {}).get('name', f\"Document {i+1}\")\n",
    "        entity_count = result.get('entity_extraction', {}).get('count', 0)\n",
    "        processing_time = result.get('total_processing_time', 0)\n",
    "        print(f\"- {doc_name}: {entity_count} entities in {processing_time:.2f} seconds\")\n",
    "else:\n",
    "    print(\"Not enough documents for batch processing demonstration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary\n",
    "\n",
    "In this notebook, we've demonstrated:\n",
    "\n",
    "1. Building a configurable OCR-NLP pipeline\n",
    "2. Processing documents to extract text and entities\n",
    "3. Generating structured JSON output\n",
    "4. Batch processing capabilities\n",
    "\n",
    "The pipeline can be extended with additional entity types, custom preprocessing steps, and domain-specific extractors for different document types."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
